{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../alzheimer_masked_nn/alzheimers_disease_data.csv'\n",
    "data = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (2149, 35)\n",
      "\n",
      "First few rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholConsumption</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>DietQuality</th>\n",
       "      <th>...</th>\n",
       "      <th>MemoryComplaints</th>\n",
       "      <th>BehavioralProblems</th>\n",
       "      <th>ADL</th>\n",
       "      <th>Confusion</th>\n",
       "      <th>Disorientation</th>\n",
       "      <th>PersonalityChanges</th>\n",
       "      <th>DifficultyCompletingTasks</th>\n",
       "      <th>Forgetfulness</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>DoctorInCharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4751</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22.927749</td>\n",
       "      <td>0</td>\n",
       "      <td>13.297218</td>\n",
       "      <td>6.327112</td>\n",
       "      <td>1.347214</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.725883</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>XXXConfid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4752</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.827681</td>\n",
       "      <td>0</td>\n",
       "      <td>4.542524</td>\n",
       "      <td>7.619885</td>\n",
       "      <td>0.518767</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.592424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>XXXConfid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4753</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17.795882</td>\n",
       "      <td>0</td>\n",
       "      <td>19.555085</td>\n",
       "      <td>7.844988</td>\n",
       "      <td>1.826335</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.119548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>XXXConfid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4754</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.800817</td>\n",
       "      <td>1</td>\n",
       "      <td>12.209266</td>\n",
       "      <td>8.428001</td>\n",
       "      <td>7.435604</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.481226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>XXXConfid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4755</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.716974</td>\n",
       "      <td>0</td>\n",
       "      <td>18.454356</td>\n",
       "      <td>6.310461</td>\n",
       "      <td>0.795498</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>XXXConfid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PatientID  Age  Gender  Ethnicity  EducationLevel        BMI  Smoking  \\\n",
       "0       4751   73       0          0               2  22.927749        0   \n",
       "1       4752   89       0          0               0  26.827681        0   \n",
       "2       4753   73       0          3               1  17.795882        0   \n",
       "3       4754   74       1          0               1  33.800817        1   \n",
       "4       4755   89       0          0               0  20.716974        0   \n",
       "\n",
       "   AlcoholConsumption  PhysicalActivity  DietQuality  ...  MemoryComplaints  \\\n",
       "0           13.297218          6.327112     1.347214  ...                 0   \n",
       "1            4.542524          7.619885     0.518767  ...                 0   \n",
       "2           19.555085          7.844988     1.826335  ...                 0   \n",
       "3           12.209266          8.428001     7.435604  ...                 0   \n",
       "4           18.454356          6.310461     0.795498  ...                 0   \n",
       "\n",
       "   BehavioralProblems       ADL  Confusion  Disorientation  \\\n",
       "0                   0  1.725883          0               0   \n",
       "1                   0  2.592424          0               0   \n",
       "2                   0  7.119548          0               1   \n",
       "3                   1  6.481226          0               0   \n",
       "4                   0  0.014691          0               0   \n",
       "\n",
       "   PersonalityChanges  DifficultyCompletingTasks  Forgetfulness  Diagnosis  \\\n",
       "0                   0                          1              0          0   \n",
       "1                   0                          0              1          0   \n",
       "2                   0                          1              0          0   \n",
       "3                   0                          0              0          0   \n",
       "4                   1                          1              0          0   \n",
       "\n",
       "   DoctorInCharge  \n",
       "0       XXXConfid  \n",
       "1       XXXConfid  \n",
       "2       XXXConfid  \n",
       "3       XXXConfid  \n",
       "4       XXXConfid  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2149 entries, 0 to 2148\n",
      "Data columns (total 35 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   PatientID                  2149 non-null   int64  \n",
      " 1   Age                        2149 non-null   int64  \n",
      " 2   Gender                     2149 non-null   int64  \n",
      " 3   Ethnicity                  2149 non-null   int64  \n",
      " 4   EducationLevel             2149 non-null   int64  \n",
      " 5   BMI                        2149 non-null   float64\n",
      " 6   Smoking                    2149 non-null   int64  \n",
      " 7   AlcoholConsumption         2149 non-null   float64\n",
      " 8   PhysicalActivity           2149 non-null   float64\n",
      " 9   DietQuality                2149 non-null   float64\n",
      " 10  SleepQuality               2149 non-null   float64\n",
      " 11  FamilyHistoryAlzheimers    2149 non-null   int64  \n",
      " 12  CardiovascularDisease      2149 non-null   int64  \n",
      " 13  Diabetes                   2149 non-null   int64  \n",
      " 14  Depression                 2149 non-null   int64  \n",
      " 15  HeadInjury                 2149 non-null   int64  \n",
      " 16  Hypertension               2149 non-null   int64  \n",
      " 17  SystolicBP                 2149 non-null   int64  \n",
      " 18  DiastolicBP                2149 non-null   int64  \n",
      " 19  CholesterolTotal           2149 non-null   float64\n",
      " 20  CholesterolLDL             2149 non-null   float64\n",
      " 21  CholesterolHDL             2149 non-null   float64\n",
      " 22  CholesterolTriglycerides   2149 non-null   float64\n",
      " 23  MMSE                       2149 non-null   float64\n",
      " 24  FunctionalAssessment       2149 non-null   float64\n",
      " 25  MemoryComplaints           2149 non-null   int64  \n",
      " 26  BehavioralProblems         2149 non-null   int64  \n",
      " 27  ADL                        2149 non-null   float64\n",
      " 28  Confusion                  2149 non-null   int64  \n",
      " 29  Disorientation             2149 non-null   int64  \n",
      " 30  PersonalityChanges         2149 non-null   int64  \n",
      " 31  DifficultyCompletingTasks  2149 non-null   int64  \n",
      " 32  Forgetfulness              2149 non-null   int64  \n",
      " 33  Diagnosis                  2149 non-null   int64  \n",
      " 34  DoctorInCharge             2149 non-null   object \n",
      "dtypes: float64(12), int64(22), object(1)\n",
      "memory usage: 587.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlzheimersDataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def preprocess(self, data, is_training=True):\n",
    "        \"\"\"\n",
    "        Preprocess the Alzheimer's dataset\n",
    "        \n",
    "        Parameters:\n",
    "        data (pd.DataFrame): Raw dataset\n",
    "        is_training (bool): Whether this is training data or prediction data\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (processed_features, processed_target) for training\n",
    "               or processed_features for prediction\n",
    "        \"\"\"\n",
    "        # Create a copy to avoid modifying original data\n",
    "        df = data.copy()\n",
    "        \n",
    "        # 1. Remove non-predictive columns\n",
    "        df = df.drop(['PatientID', 'DoctorInCharge'], axis=1)\n",
    "        \n",
    "        # 2. Separate features and target\n",
    "        if 'Diagnosis' in df.columns:\n",
    "            y = df['Diagnosis']\n",
    "            X = df.drop('Diagnosis', axis=1)\n",
    "        else:\n",
    "            X = df\n",
    "            y = None\n",
    "            \n",
    "        # 3. Define column groups\n",
    "        numerical_columns = [\n",
    "            'Age', 'BMI', 'AlcoholConsumption', 'PhysicalActivity',\n",
    "            'DietQuality', 'SleepQuality', 'SystolicBP', 'DiastolicBP',\n",
    "            'CholesterolTotal', 'CholesterolLDL', 'CholesterolHDL',\n",
    "            'CholesterolTriglycerides', 'MMSE', 'FunctionalAssessment', 'ADL'\n",
    "        ]\n",
    "        \n",
    "        # Already encoded categorical columns (no processing needed)\n",
    "        categorical_columns = ['Gender', 'Ethnicity', 'EducationLevel']\n",
    "        \n",
    "        # Binary columns (already 0/1)\n",
    "        binary_columns = [\n",
    "            'Smoking', 'FamilyHistoryAlzheimers', 'CardiovascularDisease',\n",
    "            'Diabetes', 'Depression', 'HeadInjury', 'Hypertension',\n",
    "            'MemoryComplaints', 'BehavioralProblems', 'Confusion',\n",
    "            'Disorientation', 'PersonalityChanges', 'DifficultyCompletingTasks',\n",
    "            'Forgetfulness'\n",
    "        ]\n",
    "        \n",
    "        # 4. Scale numerical features\n",
    "        for col in numerical_columns:\n",
    "            if is_training:\n",
    "                self.scalers[col] = StandardScaler()\n",
    "                X[col] = self.scalers[col].fit_transform(X[col].values.reshape(-1, 1))\n",
    "            else:\n",
    "                X[col] = self.scalers[col].transform(X[col].values.reshape(-1, 1))\n",
    "        \n",
    "        # 5. Create mask for missing values\n",
    "        mask = ~X.isna()\n",
    "        \n",
    "        # 6. Fill missing values with 0 (they'll be masked anyway)\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        if y is not None:\n",
    "            return X, mask, y\n",
    "        return X, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for training\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): Input dataframe\n",
    "    test_size (float): Proportion of data to use for testing\n",
    "    random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_train, X_test, masks_train, masks_test, y_train, y_test, preprocessor)\n",
    "    \"\"\"\n",
    "    # Create preprocessor\n",
    "    preprocessor = AlzheimersDataPreprocessor()\n",
    "    \n",
    "    # Split data\n",
    "    train_data, test_data = train_test_split(\n",
    "        data, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=data['Diagnosis']\n",
    "    )\n",
    "    \n",
    "    # Preprocess training data\n",
    "    X_train, masks_train, y_train = preprocessor.preprocess(train_data, is_training=True)\n",
    "    \n",
    "    # Preprocess test data using fitted preprocessor\n",
    "    X_test, masks_test, y_test = preprocessor.preprocess(test_data, is_training=False)\n",
    "    \n",
    "    return (X_train, X_test, masks_train, masks_test, \n",
    "            y_train, y_test, preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Data Shapes:\n",
      "X_train shape: (1719, 32)\n",
      "X_test shape: (430, 32)\n",
      "masks_train shape: (1719, 32)\n",
      "masks_test shape: (430, 32)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "X_train, X_test, masks_train, masks_test, y_train, y_test, preprocessor = prepare_data(data)\n",
    "\n",
    "# Print information about the processed data\n",
    "print(\"Processed Data Shapes:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"masks_train shape: {masks_train.shape}\")\n",
    "print(f\"masks_test shape: {masks_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of scaled numerical features:\n",
      "           Age       BMI  AlcoholConsumption  PhysicalActivity  DietQuality  \\\n",
      "780   1.230721  0.065466            1.478214          0.571099    -1.276449   \n",
      "1341 -1.430841  0.070065           -0.407432         -1.050032     0.340303   \n",
      "820   0.010838 -0.316941           -0.288334          1.628134    -1.119152   \n",
      "857   1.563416 -0.567289           -1.418146         -1.257326     1.279730   \n",
      "593   0.121737  1.533521           -0.190409          1.188813     0.982253   \n",
      "\n",
      "      SleepQuality  SystolicBP  DiastolicBP  CholesterolTotal  CholesterolLDL  \\\n",
      "780       1.457443    0.195308    -0.234958          0.059480       -1.656832   \n",
      "1341     -1.135747    1.622491     1.200748          1.498922        1.190810   \n",
      "820       0.512803   -0.498997     1.545317          0.019782        0.414247   \n",
      "857       1.255038    1.391056    -1.096381         -0.159480       -0.970426   \n",
      "593      -0.632116   -0.614714    -0.522099         -1.634594       -0.220092   \n",
      "\n",
      "      CholesterolHDL  CholesterolTriglycerides      MMSE  \\\n",
      "780         0.431935                  0.580991  0.688179   \n",
      "1341       -0.043460                  1.320300 -0.667390   \n",
      "820        -1.533359                 -0.732406 -0.318460   \n",
      "857         1.561620                  0.794770  1.508058   \n",
      "593        -1.135127                 -0.750147  1.247306   \n",
      "\n",
      "      FunctionalAssessment       ADL  \n",
      "780               0.153928 -1.603337  \n",
      "1341              1.434705 -1.532595  \n",
      "820              -0.381928 -0.020447  \n",
      "857               1.568971  0.060314  \n",
      "593               1.201588  1.498464  \n"
     ]
    }
   ],
   "source": [
    "# Display sample of processed numerical features\n",
    "numerical_columns = [\n",
    "    'Age', 'BMI', 'AlcoholConsumption', 'PhysicalActivity',\n",
    "    'DietQuality', 'SleepQuality', 'SystolicBP', 'DiastolicBP',\n",
    "    'CholesterolTotal', 'CholesterolLDL', 'CholesterolHDL',\n",
    "    'CholesterolTriglycerides', 'MMSE', 'FunctionalAssessment', 'ADL'\n",
    "]\n",
    "print(\"Sample of scaled numerical features:\")\n",
    "print(X_train[numerical_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of categorical features (already encoded):\n",
      "      Gender  Ethnicity  EducationLevel\n",
      "780        0          2               1\n",
      "1341       1          0               3\n",
      "820        0          0               0\n",
      "857        0          0               0\n",
      "593        1          0               2\n"
     ]
    }
   ],
   "source": [
    "# Display sample of categorical features (already encoded)\n",
    "categorical_columns = ['Gender', 'Ethnicity', 'EducationLevel']\n",
    "print(\"Sample of categorical features (already encoded):\")\n",
    "print(X_train[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      "Training set:\n",
      "Diagnosis\n",
      "0    0.646306\n",
      "1    0.353694\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set:\n",
      "Diagnosis\n",
      "0    0.646512\n",
      "1    0.353488\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(\"Training set:\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "print(\"\\nTest set:\")\n",
    "print(pd.Series(y_test).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\alzheimer_masked_nn\\alzheimers_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingValueNetwork:\n",
    "    def __init__(self, input_dim, hidden_dims=[64, 32]):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "\n",
    "        # Main input for features\n",
    "\n",
    "        main_input = layers.Input(shape=(self.input_dim,), name='main_input')\n",
    "\n",
    "\n",
    "        # Mask input (1 for present, 0 for missing)\n",
    "\n",
    "        mask_input = layers.Input(shape=(self.input_dim,), name='mask_input')\n",
    "\n",
    "\n",
    "        # Branch 1: Process available values\n",
    "\n",
    "        masked_input = layers.Multiply()([main_input, mask_input])\n",
    "\n",
    "\n",
    "        x1 = masked_input\n",
    "\n",
    "        for dim in self.hidden_dims:\n",
    "\n",
    "            x1 = layers.Dense(dim, activation='relu')(x1)\n",
    "\n",
    "            x1 = layers.BatchNormalization()(x1)\n",
    "\n",
    "            x1 = layers.Dropout(0.3)(x1)\n",
    "\n",
    "\n",
    "        # Branch 2: Process missing patterns\n",
    "\n",
    "        x2 = mask_input\n",
    "\n",
    "        for dim in self.hidden_dims:\n",
    "\n",
    "            x2 = layers.Dense(dim//2, activation='relu')(x2)\n",
    "\n",
    "            x2 = layers.BatchNormalization()(x2)\n",
    "\n",
    "            x2 = layers.Dropout(0.3)(x2)\n",
    "\n",
    "\n",
    "        # Combine both branches\n",
    "\n",
    "        combined = layers.Concatenate()([x1, x2])\n",
    "\n",
    "\n",
    "        # Final processing\n",
    "\n",
    "        x = layers.Dense(32, activation='relu')(combined)\n",
    "\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "\n",
    "\n",
    "        # Output layer\n",
    "\n",
    "        output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "        # Create model\n",
    "\n",
    "        model = Model(inputs=[main_input, mask_input], outputs=output)\n",
    "\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "\n",
    "            optimizer='adam',\n",
    "\n",
    "            loss='binary_crossentropy',\n",
    "\n",
    "            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def train(self, X_train, masks_train, y_train, validation_data, \n",
    "\n",
    "              epochs=50, batch_size=32, class_weights=None):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Train the model\n",
    "        \n",
    "\n",
    "        Parameters:\n",
    "\n",
    "        X_train: Training features\n",
    "\n",
    "        masks_train: Training masks\n",
    "\n",
    "        y_train: Training labels\n",
    "\n",
    "        validation_data: Tuple of (X_val, masks_val, y_val)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare validation data\n",
    "\n",
    "        X_val, masks_val, y_val = validation_data\n",
    "\n",
    "\n",
    "        # Add early stopping\n",
    "\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "\n",
    "            monitor='val_auc',\n",
    "\n",
    "            patience=10,\n",
    "\n",
    "            mode='max',\n",
    "\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "\n",
    "        # Add model checkpoint\n",
    "\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            \"best_model.keras\", monitor=\"val_auc\", mode=\"max\", save_best_only=True\n",
    "        )\n",
    "\n",
    "\n",
    "        # Create TQDM callback for progress bar\n",
    "\n",
    "        class TqdmCallback(tf.keras.callbacks.Callback):\n",
    "            def __init__(self, epochs):\n",
    "                super().__init__()\n",
    "                self.epochs = epochs\n",
    "                self.progress_bar = None\n",
    "\n",
    "            def on_train_begin(self, logs=None):\n",
    "                self.progress_bar = tqdm(total=self.epochs, desc=\"Training\")\n",
    "\n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                self.progress_bar.update(1)\n",
    "                self.progress_bar.set_postfix(\n",
    "                    {\n",
    "                        \"loss\": f\"{logs['loss']:.4f}\",\n",
    "                        \"acc\": f\"{logs['accuracy']:.4f}\",\n",
    "                        \"val_loss\": f\"{logs['val_loss']:.4f}\",\n",
    "                        \"val_acc\": f\"{logs['val_accuracy']:.4f}\",\n",
    "                        'auc': f\"{logs.get('auc', 0):.4f}\"\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            def on_train_end(self, logs=None):\n",
    "                self.progress_bar.close()\n",
    "\n",
    "\n",
    "        # Train model with TQDM progress bar\n",
    "\n",
    "        history = self.model.fit(\n",
    "\n",
    "            [X_train, masks_train],\n",
    "            y_train,\n",
    "\n",
    "            validation_data=([X_val, masks_val], y_val),\n",
    "\n",
    "            epochs=epochs,\n",
    "\n",
    "            batch_size=batch_size,\n",
    "\n",
    "            callbacks=[early_stopping, checkpoint, TqdmCallback(epochs)],\n",
    "\n",
    "            class_weight=class_weights,\n",
    "\n",
    "            verbose=0  # Set to 0 since we're using TQDM\n",
    "        )\n",
    "\n",
    "\n",
    "        return history\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, masks_test, y_test):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Evaluate the model\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Get predictions\n",
    "\n",
    "        y_pred_proba = self.model.predict([X_test, masks_test])\n",
    "\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "\n",
    "        # Print classification report\n",
    "\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "        # Create confusion matrix\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "        # Plot confusion matrix\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "        plt.title('Confusion Matrix')\n",
    "\n",
    "        plt.ylabel('True Label')\n",
    "\n",
    "        plt.xlabel('Predicted Label')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        return y_pred_proba, y_pred\n",
    "\n",
    "\n",
    "    def plot_training_history(self, history):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Plot training history\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "\n",
    "        # Plot accuracy\n",
    "\n",
    "        ax1.plot(history.history['accuracy'])\n",
    "\n",
    "        ax1.plot(history.history['val_accuracy'])\n",
    "\n",
    "        ax1.set_title('Model Accuracy')\n",
    "\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "\n",
    "        ax1.set_xlabel('Epoch')\n",
    "\n",
    "        ax1.legend(['Train', 'Validation'])\n",
    "\n",
    "\n",
    "        # Plot loss\n",
    "\n",
    "        ax2.plot(history.history['loss'])\n",
    "\n",
    "        ax2.plot(history.history['val_loss'])\n",
    "\n",
    "        ax2.set_title('Model Loss')\n",
    "\n",
    "        ax2.set_ylabel('Loss')\n",
    "\n",
    "        ax2.set_xlabel('Epoch')\n",
    "\n",
    "        ax2.legend(['Train', 'Validation'])\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in training data: [0 1]\n",
      "\n",
      "Class distribution in training data:\n",
      "Diagnosis\n",
      "0    1111\n",
      "1     608\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class weights: {0: np.float64(0.7736273627362736), 1: np.float64(1.4136513157894737)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training:   0%|          | 0/50 [03:38<?, ?it/s]\n",
      "Training:   0%|          | 0/50 [01:15<?, ?it/s]\n",
      "e:\\alzheimer_masked_nn\\alzheimers_env\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: accuracy,auc_3,loss,val_accuracy,val_auc_3,val_loss\n",
      "  current = self.get_monitor_value(logs)\n",
      "e:\\alzheimer_masked_nn\\alzheimers_env\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:206: UserWarning: Can save best model only with val_auc available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Training: 100%|██████████| 50/50 [00:25<00:00,  1.99it/s, loss=0.2894, acc=0.8831, val_loss=0.3880, val_acc=0.8605]\n"
     ]
    }
   ],
   "source": [
    "# Print class distribution\n",
    "print(\"Unique classes in training data:\", np.unique(y_train))\n",
    "print(\"\\nClass distribution in training data:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "# Convert y_train and y_test to numpy arrays if they're pandas Series\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Calculate class weights\n",
    "unique_classes = np.unique(y_train)\n",
    "n_samples = len(y_train)\n",
    "class_weights = dict(\n",
    "    enumerate(n_samples / (len(unique_classes) * np.bincount(y_train)))\n",
    ")\n",
    "\n",
    "print(\"\\nClass weights:\", class_weights)\n",
    "\n",
    "# Train the model\n",
    "history = model.train(\n",
    "    X_train,\n",
    "    masks_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, masks_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weights=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m     class_weights \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28mcls\u001b[39m: n_samples \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(unique_classes) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(y_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mcls\u001b[39m))\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m unique_classes\n\u001b[0;32m     13\u001b[0m     }\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasks_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 101\u001b[0m, in \u001b[0;36mMissingValueNetwork.train\u001b[1;34m(self, X_train, masks_train, y_train, validation_data, epochs, batch_size, class_weights)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_bar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Train model with TQDM progress bar\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTqdmCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to 0 since we're using TQDM\u001b[39;49;00m\n\u001b[0;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32me:\\alzheimer_masked_nn\\alzheimers_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\alzheimer_masked_nn\\alzheimers_env\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32me:\\alzheimer_masked_nn\\alzheimers_env\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32me:\\alzheimer_masked_nn\\alzheimers_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# Initialize model with the correct input dimension\n",
    "input_dim = X_train.shape[1]\n",
    "model = MissingValueNetwork(input_dim=input_dim)\n",
    "\n",
    "# Calculate class weights if dataset is imbalanced\n",
    "class_weights = None\n",
    "if len(set(y_train)) > 1:\n",
    "    unique_classes = np.unique(y_train)\n",
    "    n_samples = len(y_train)\n",
    "    class_weights = {\n",
    "        cls: n_samples / (len(unique_classes) * sum(y_train == cls))\n",
    "        for cls in unique_classes\n",
    "    }\n",
    "\n",
    "# Train the model\n",
    "history = model.train(\n",
    "    X_train,\n",
    "    masks_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, masks_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weights=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "model.plot_training_history(history)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_proba, y_pred = model.evaluate(X_test, masks_test, y_test)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Make Predictions for New Data\n",
    "\n",
    "# %%\n",
    "def predict_new_cases(model, preprocessor, new_data):\n",
    "    \"\"\"\n",
    "    Make predictions for new cases\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained MissingValueNetwork\n",
    "    preprocessor: Trained AlzheimersDataPreprocessor\n",
    "    new_data: DataFrame with new cases\n",
    "    \"\"\"\n",
    "    # Preprocess new data\n",
    "    X_new, masks_new = preprocessor.preprocess(new_data, is_training=False)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_proba = model.model.predict([X_new, masks_new])\n",
    "    predictions = (predictions_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'Predicted_Probability': predictions_proba.flatten(),\n",
    "        'Predicted_Class': predictions.flatten()\n",
    "    })\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alzheimers_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
